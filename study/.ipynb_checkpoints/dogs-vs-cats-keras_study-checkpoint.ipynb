{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9619b018-f7a0-4d13-90b4-f769a8906003",
    "_uuid": "cac9915711fdcec7e8eb6431fd8dcae21649b6aa"
   },
   "source": [
    "**Building a strong image classification model from less data**\n",
    "\n",
    "The implementation is a slight variation of the one in https://gist.github.com/fchollet/0830affa1f7f19fd47b06d4cf89ed44d\n",
    "\n",
    "Mainly, in this kernel , the method flow(x,y) is used whereas, in the above gist, method flow_from_directory(directory) is used.\n",
    "For more info, you can refer https://keras.io/preprocessing/image/\n",
    "\n",
    "The change is made to have an appropriate kernel to deal with the way data is structured in kaggle. Appropriate changes in other parts of the source code is also done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e10726c1-a487-4d89-bf8f-7ff532592440",
    "_uuid": "16fefbb9857aead05fe80ecbf3ffbabfae4fca22"
   },
   "source": [
    "**Perform the necessary imports.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "4f4ac6a4-708c-46ab-8269-e01978300bde",
    "_uuid": "49532c85c74dee05663bd7eda324d3df493c60ed"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, cv2, re, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "from keras import layers, models, optimizers\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5fa55078-4136-47a1-9914-1cac54eec440",
    "_uuid": "d57ed5db51e8c287e62a8b40d264c42ca64a9ecf"
   },
   "source": [
    "**Data dimensions and paths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "25980d83-1f66-420b-a1dc-501699c3d707",
    "_uuid": "2eb716c4e9ad7d0bd7293f0f156b5a751263b800"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E:\\\\kaggle\\\\dogs-vs-cats\\\\train\\\\cat.1.jpg', 'E:\\\\kaggle\\\\dogs-vs-cats\\\\train\\\\cat.10.jpg', 'E:\\\\kaggle\\\\dogs-vs-cats\\\\train\\\\cat.100.jpg', 'E:\\\\kaggle\\\\dogs-vs-cats\\\\train\\\\cat.1000.jpg']\n",
      "\n",
      "['E:\\\\kaggle\\\\dogs-vs-cats\\\\test\\\\10.jpg', 'E:\\\\kaggle\\\\dogs-vs-cats\\\\test\\\\100.jpg', 'E:\\\\kaggle\\\\dogs-vs-cats\\\\test\\\\1000.jpg', 'E:\\\\kaggle\\\\dogs-vs-cats\\\\test\\\\10000.jpg']\n",
      "\n",
      "25000\n",
      "12500\n"
     ]
    }
   ],
   "source": [
    "img_width = 150\n",
    "img_height = 150\n",
    "TRAIN_DIR = 'E:\\\\kaggle\\\\dogs-vs-cats\\\\train\\\\'\n",
    "TEST_DIR = 'E:\\\\kaggle\\\\dogs-vs-cats\\\\test\\\\'\n",
    "\n",
    "train_images_dogs_cats = [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR)] # use this for full dataset\n",
    "test_images_dogs_cats = [TEST_DIR+i for i in os.listdir(TEST_DIR)]\n",
    "\n",
    "# 현재 정렬이 되어있지는 않다.\n",
    "print(train_images_dogs_cats[1:5], end = '\\n\\n')\n",
    "print(test_images_dogs_cats[1:5], end = '\\n\\n')\n",
    "\n",
    "print(len(train_images_dogs_cats))\n",
    "print(len(test_images_dogs_cats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7fd765e1-e4e1-4b59-b7b2-33336afd4848",
    "_uuid": "505895b51eb3ed5a1dee521c9adc0864639d0b96"
   },
   "source": [
    "**Helper function to sort the image files based on the numeric value in each file name.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "a11beea5-afc8-42ac-9688-c9bd45ea19aa",
    "_uuid": "775ea273467ff9d32fbf2c1b2158b57f473618b5"
   },
   "outputs": [],
   "source": [
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "# text에 숫자만 있으면 int형으로 반환하고\n",
    "# text에 숫자만 있지 않으면 그냥 text 그대로 반환해라\n",
    "\n",
    "def natural_keys(text):\n",
    "    return [ atoi(c) for c in re.split('(\\d+)', text) ]\n",
    "\n",
    "# 파이썬 정규 표현식 re.split을 사용\n",
    "# ('\\d+') : 숫자가 아닌것들이 한번 이상 반복을 뜻함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show atoi & natural_keys functions examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dog.', 12483, '.jpg']\n"
     ]
    }
   ],
   "source": [
    "# Show atoi & natural_keys functions examples \n",
    "\n",
    "print (natural_keys('dog.12483.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d4c8b492-315c-4c38-9b1e-32c05e368027",
    "_uuid": "11a7c4072cdedd8f5b79d4bfcec3d1f31d448404"
   },
   "source": [
    "**Sort the traning set. Use 1300 images each of cats and dogs instead of all 25000 to speed up the learning process.**\n",
    "\n",
    "**Sort the test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "001b4ea8-a12f-4290-9a0d-6990b7deb2f7",
    "_uuid": "22c6c18de15a48f12e9f2120ea56f6e603bc6329"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E:\\\\kaggle\\\\dogs-vs-cats\\\\train\\\\cat.0.jpg', 'E:\\\\kaggle\\\\dogs-vs-cats\\\\train\\\\cat.1.jpg', 'E:\\\\kaggle\\\\dogs-vs-cats\\\\train\\\\cat.2.jpg', 'E:\\\\kaggle\\\\dogs-vs-cats\\\\train\\\\cat.3.jpg', 'E:\\\\kaggle\\\\dogs-vs-cats\\\\train\\\\cat.4.jpg']\n"
     ]
    }
   ],
   "source": [
    "train_images_dogs_cats.sort(key=natural_keys)\n",
    "# sort로 정렬을 하는데 natural_keys의 함수의 결과에 따라 정렬한다.\n",
    "\n",
    "# results example\n",
    "# 정렬이 제대로 되어서 결과가 나온다.\n",
    "print(train_images_dogs_cats[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 과정속도를 늘리기 위해서 25000개 image데이터를 다 사용하지는 않고 1300개 개와 고양이의 image_data만 추려낼 것이다.\n",
    "# list합치기를 이용\n",
    "train_images_dogs_cats = train_images_dogs_cats[0:1300] + train_images_dogs_cats[12500:13800] \n",
    "\n",
    "# 다시 정렬하기\n",
    "test_images_dogs_cats.sort(key=natural_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "58d15bea-7674-46ff-ba0d-af6a920590be",
    "_uuid": "5d3a332070aadd998dcb3ac506f086d1cbe37b20"
   },
   "source": [
    "**Now the images have to be represented in numbers. For this, using the openCV library read and resize the image.  **\n",
    "\n",
    "**Generate labels for the supervised learning set.**\n",
    "\n",
    "**Below is the helper function to do so.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "be6eb2de-4302-4731-ba11-75cdb114a0df",
    "_uuid": "5dcd4727d558b622fa5533f203834e86c758dace",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(list_of_images):\n",
    "    \"\"\"\n",
    "    Returns two arrays: \n",
    "        x is an array of resized images\n",
    "        y is an array of labels\n",
    "    \"\"\"\n",
    "    x = [] # images as arrays\n",
    "    y = [] # labels\n",
    "    \n",
    "    for image in list_of_images:\n",
    "        x.append(cv2.resize(cv2.imread(image), (img_width,img_height), interpolation=cv2.INTER_CUBIC))\n",
    "    \n",
    "    for i in list_of_images:\n",
    "        if 'dog' in i:\n",
    "            y.append(1)\n",
    "        elif 'cat' in i:\n",
    "            y.append(0)\n",
    "        #else:\n",
    "            #print('neither cat nor dog name present in images')\n",
    "            \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "eaddc700-7124-4526-9615-46e96144af58",
    "_uuid": "c42578104846f709ec064b3fa82c861597dd18f1"
   },
   "source": [
    "**Generate X and Y using the helper function above**\n",
    "\n",
    "**Since K.image_data_format() is channel_last,  input_shape to the first keras layer will be (img_width, img_height, 3). '3' since it is a color image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "9add14aa-3cd6-4160-a3f3-20a2f6b61af9",
    "_uuid": "0111553c02dd7e4d622aeb65c3d307038f32f000",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channels_last\n"
     ]
    }
   ],
   "source": [
    "X, Y = prepare_data(train_images_dogs_cats)\n",
    "print(K.image_data_format())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "110e7b06-d53d-4be4-ae4f-ca62a9beed89",
    "_uuid": "13d1080aa11311bd3fe29857f621503797111080"
   },
   "source": [
    "**Split the data set containing 2600 images into 2 parts, training set and validation set. Later, you will see that accuracy and loss on the validation set will also be reported while fitting the model using training set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "6a94930d-9c33-4361-ba3d-ef1f29707bce",
    "_uuid": "26eea9b27bcbf6a0ca3eba6f91cf5cace580f442",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First split the data in two sets, 80% for training, 20% for Val/Test)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X,Y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "34977dad-9c51-4946-a465-d79138306b03",
    "_uuid": "c5e3ba21cd44491307721204db024166a5028499",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_train_samples = len(X_train)\n",
    "nb_validation_samples = len(X_val)\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f4cfc349-2ad8-46f8-9816-80b086401eff",
    "_uuid": "6b998b641c185a10e809d95762b57a13a0065f56"
   },
   "source": [
    "**We will be using the Sequential model from Keras to form the Neural Network. Sequential Model is  used to construct simple models with linear stack of layers. **\n",
    "\n",
    "**More info on Sequential model and Keras in general at https://keras.io/getting-started/sequential-model-guide/ and https://github.com/keras-team/keras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "e72ad7e4-0692-4dd2-a49f-47640caf2e52",
    "_uuid": "f27c9488fc9767db7ef6beb75b45959852e6a018",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 148, 148, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 72, 72, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 72, 72, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 34, 34, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 34, 34, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 18496)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                1183808   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,212,513\n",
      "Trainable params: 1,212,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(32, (3, 3), input_shape=(img_width, img_height, 3)))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(32, (3, 3)))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3)))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1))\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3342dd4d-9dc1-454a-9f09-0674c8ac41d1",
    "_uuid": "cc941a4d3d7cef29c74b4c28bb8849c2ebae5a40"
   },
   "source": [
    "**This is the augmentation configuration we will use for training and validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "cf7cd57a-4532-4d79-a2dc-431b7bf174eb",
    "_uuid": "8a40cd117597246908aa7ef02172d08096548ba2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "val_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1fffe8d6-e21b-4354-8313-323bfbb517da",
    "_uuid": "c78ba8af934502e7a736e1671a4ef73452ace722"
   },
   "source": [
    "**Prepare generators for training and validation sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "36388127-2d62-4aac-999e-04decf9bba30",
    "_uuid": "3b91d6026f7a71f7a5f8347032fd03c6dc929337",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow(np.array(X_train), Y_train, batch_size=batch_size)\n",
    "validation_generator = val_datagen.flow(np.array(X_val), Y_val, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "00cab697-e7c4-41a3-9014-1f7406c2d104",
    "_uuid": "bf95bd03922a7449939b135c632079968487324f"
   },
   "source": [
    "**Start training the model!**\n",
    "\n",
    "**For better accuracy and lower loss, we are using an epoch of 30. Epoch value can be increased for better results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "446727ad-a903-45c2-bf87-97152984baa9",
    "_uuid": "d46dd6f0d43b62a27e92fe1c0088d787709fc584",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "130/130 [==============================] - 66s 511ms/step - loss: 0.6975 - acc: 0.5322 - val_loss: 0.6780 - val_acc: 0.6562\n",
      "Epoch 2/30\n",
      "130/130 [==============================] - 65s 497ms/step - loss: 0.6697 - acc: 0.6188 - val_loss: 0.6332 - val_acc: 0.6448\n",
      "Epoch 3/30\n",
      "130/130 [==============================] - 65s 496ms/step - loss: 0.6189 - acc: 0.6620 - val_loss: 0.5791 - val_acc: 0.6885\n",
      "Epoch 4/30\n",
      "130/130 [==============================] - 64s 494ms/step - loss: 0.5926 - acc: 0.6986 - val_loss: 0.5437 - val_acc: 0.7282\n",
      "Epoch 5/30\n",
      "130/130 [==============================] - 65s 503ms/step - loss: 0.5668 - acc: 0.7120 - val_loss: 0.5757 - val_acc: 0.7143\n",
      "Epoch 6/30\n",
      "130/130 [==============================] - 65s 504ms/step - loss: 0.5497 - acc: 0.7337 - val_loss: 0.5290 - val_acc: 0.7262\n",
      "Epoch 7/30\n",
      "130/130 [==============================] - 66s 507ms/step - loss: 0.5339 - acc: 0.7486 - val_loss: 0.5963 - val_acc: 0.6925\n",
      "Epoch 8/30\n",
      "130/130 [==============================] - 65s 501ms/step - loss: 0.5297 - acc: 0.7510 - val_loss: 0.5193 - val_acc: 0.7540\n",
      "Epoch 9/30\n",
      "130/130 [==============================] - 65s 502ms/step - loss: 0.5195 - acc: 0.7510 - val_loss: 0.6110 - val_acc: 0.7183\n",
      "Epoch 10/30\n",
      "130/130 [==============================] - 66s 507ms/step - loss: 0.4811 - acc: 0.7856 - val_loss: 0.5405 - val_acc: 0.7143\n",
      "Epoch 11/30\n",
      "130/130 [==============================] - 65s 503ms/step - loss: 0.5019 - acc: 0.7673 - val_loss: 0.4697 - val_acc: 0.7698\n",
      "Epoch 12/30\n",
      "130/130 [==============================] - 66s 508ms/step - loss: 0.4755 - acc: 0.7817 - val_loss: 0.6739 - val_acc: 0.6925\n",
      "Epoch 13/30\n",
      "130/130 [==============================] - 65s 503ms/step - loss: 0.4534 - acc: 0.7904 - val_loss: 0.5238 - val_acc: 0.7440\n",
      "Epoch 14/30\n",
      "130/130 [==============================] - 66s 505ms/step - loss: 0.4590 - acc: 0.7841 - val_loss: 0.4887 - val_acc: 0.7500\n",
      "Epoch 15/30\n",
      "130/130 [==============================] - 62s 474ms/step - loss: 0.4603 - acc: 0.8019 - val_loss: 0.4493 - val_acc: 0.7917\n",
      "Epoch 16/30\n",
      "130/130 [==============================] - 57s 441ms/step - loss: 0.4374 - acc: 0.8010 - val_loss: 0.5178 - val_acc: 0.7421\n",
      "Epoch 17/30\n",
      "130/130 [==============================] - 58s 442ms/step - loss: 0.4301 - acc: 0.8144 - val_loss: 0.5099 - val_acc: 0.7460\n",
      "Epoch 18/30\n",
      "130/130 [==============================] - 58s 443ms/step - loss: 0.4210 - acc: 0.8187 - val_loss: 0.4815 - val_acc: 0.7817\n",
      "Epoch 19/30\n",
      "130/130 [==============================] - 58s 447ms/step - loss: 0.4274 - acc: 0.8139 - val_loss: 0.6457 - val_acc: 0.7540\n",
      "Epoch 20/30\n",
      "130/130 [==============================] - 57s 441ms/step - loss: 0.4328 - acc: 0.8115 - val_loss: 0.4558 - val_acc: 0.7877\n",
      "Epoch 21/30\n",
      "130/130 [==============================] - 58s 444ms/step - loss: 0.4246 - acc: 0.8130 - val_loss: 0.5150 - val_acc: 0.7599\n",
      "Epoch 22/30\n",
      "130/130 [==============================] - 57s 440ms/step - loss: 0.4167 - acc: 0.8240 - val_loss: 0.5075 - val_acc: 0.7540\n",
      "Epoch 23/30\n",
      "130/130 [==============================] - 58s 448ms/step - loss: 0.3926 - acc: 0.8260 - val_loss: 0.5551 - val_acc: 0.7778\n",
      "Epoch 24/30\n",
      "130/130 [==============================] - 58s 442ms/step - loss: 0.4056 - acc: 0.8192 - val_loss: 0.5659 - val_acc: 0.7778\n",
      "Epoch 25/30\n",
      "130/130 [==============================] - 60s 464ms/step - loss: 0.4072 - acc: 0.8346 - val_loss: 0.5558 - val_acc: 0.7659\n",
      "Epoch 26/30\n",
      "130/130 [==============================] - 58s 446ms/step - loss: 0.4228 - acc: 0.8197 - val_loss: 0.5345 - val_acc: 0.7639\n",
      "Epoch 27/30\n",
      "130/130 [==============================] - 57s 435ms/step - loss: 0.3955 - acc: 0.8332 - val_loss: 0.5256 - val_acc: 0.7619\n",
      "Epoch 28/30\n",
      "130/130 [==============================] - 57s 440ms/step - loss: 0.4030 - acc: 0.8264 - val_loss: 0.5396 - val_acc: 0.7421\n",
      "Epoch 29/30\n",
      "130/130 [==============================] - 59s 452ms/step - loss: 0.3975 - acc: 0.8389 - val_loss: 0.5342 - val_acc: 0.7520\n",
      "Epoch 30/30\n",
      "130/130 [==============================] - 58s 445ms/step - loss: 0.3950 - acc: 0.8293 - val_loss: 0.5685 - val_acc: 0.7837\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    train_generator, \n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=30,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cf363841-697a-4b38-9415-a1f57b8f16cb",
    "_uuid": "8d4932c5555b6274a8a0b16c3fb770c003a1d380"
   },
   "source": [
    "**Well done for a small training set and a small Epoch! Accuracy on the training set is close to 84% while on the validation set is close to 79%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e5ba296f-049f-407a-ae30-22a2226d4810",
    "_uuid": "3d679de7accbe4df7daac93969900f4f2c8c3d75"
   },
   "source": [
    "**Saving the model in Keras is simple as this! ** \n",
    "\n",
    "**It is quite helpful for reuse.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "32aa2713-f32d-458e-9f9c-8c0dcb5f50f0",
    "_uuid": "c3dc5f39f8e8227b6fbc99dab80af96a48aa9c1b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('model_wieghts.h5')\n",
    "model.save('model_keras.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7ed718a4-826e-4b83-9f2e-2156f6a75f45",
    "_uuid": "a1889aed06ff0c9d9a4f5095cc38649c2606dbca"
   },
   "source": [
    "**Time to predict classification using the model on the test set.**\n",
    "\n",
    "**Generate X_test and Y_test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "490f5e45-eefd-4a46-b3bd-3e7e09cf695d",
    "_uuid": "f807eefa26fcb70a4b9f7eb8b6c06e50109f65fc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test, Y_test = prepare_data(test_images_dogs_cats) #Y_test in this case will be []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8a84efef-c2a3-4cda-a3a0-a6c2d6032238",
    "_uuid": "c5c594867c18faeb5053bc2c819f78514f07a493"
   },
   "source": [
    "**This is the augmentation configuration we will use for testing. Only rescaling.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "a2c141c2-ec9e-4eab-8ea8-fa1a7485e506",
    "_uuid": "33af89c39a57edfb49dca2a33c159c96e7d63fe2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1. / 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "db815324-2ca3-49ce-a005-a1578f179092",
    "_uuid": "3c074c8a7974d1c2b01d07d37a2f3bab88fc9951"
   },
   "source": [
    "**Prepare generator for test set and start predicting on it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "112ff470-4976-496c-bb0c-2c6eddddb528",
    "_uuid": "49a4cc7684dfde31cd2dfd2921bfa15bfae19672",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 146s 187ms/step\n"
     ]
    }
   ],
   "source": [
    "test_generator = val_datagen.flow(np.array(X_test), batch_size=batch_size)\n",
    "prediction_probabilities = model.predict_generator(test_generator, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ae9bb1ac-91a2-43c4-a57d-e87651c4d3e4",
    "_uuid": "e299a1c8f9a927a2896a6dacb016bba9c83d0eea"
   },
   "source": [
    "**Generate .csv for submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "bc3fada0-c12a-4b36-a3a1-eaa7195e9b19",
    "_uuid": "75ff4f7fa16b6fc9758bcea0dd2107b65740cfda",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counter = range(1, len(test_images_dogs_cats) + 1)\n",
    "solution = pd.DataFrame({\"id\": counter, \"label\":list(prediction_probabilities)})\n",
    "cols = ['label']\n",
    "\n",
    "for col in cols:\n",
    "    solution[col] = solution[col].map(lambda x: str(x).lstrip('[').rstrip(']')).astype(float)\n",
    "\n",
    "solution.to_csv(\"dogsVScats.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "edaa9c27-5d0a-4488-b67b-525f7f71377f",
    "_uuid": "f333a76d6514546f410d0e2679431515681bb87f"
   },
   "source": [
    "**Kindly upvote if you find this kernel useful**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf2.0-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
